---
title: "2021 Lab Journal"
author: "Kelly Mistry"
date: "1/26/2021"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Applied Time Series Analysis course materials

Continuing to work through Applied Time Series Analysis course materials.

#### Section Five

Box-Jenkins method steps: \* Model form selection, which entails evaluating stationarity (and if stationarity needs to be adjusted, selecting differencing level), selecting the AR level (p) and selecting the MA level (q) \* Parameter estimation \* Model checking

Stationarity tests: Dickey-Fuller (for lag of 0), Augmented Dickey-Fuller (for lags greater than 0) and KPSS

```{r ATSA Section 5, eval = FALSE, echo = FALSE}

```

#### Section Six

```{r ATSA Section 6, eval = FALSE, echo = FALSE}
```

#### Section Seven

```{r ATSA Section 7, eval = FALSE, echo = FALSE}
```

#### Section Eight

```{r ATSA Section 8, eval = FALSE, echo = FALSE}
```

#### Section Nine

```{r ATSA Section 9, eval = FALSE, echo = FALSE}
```

#### Section Ten

```{r ATSA Section 10, eval = FALSE, echo = FALSE}
```

#### Section Eleven

```{r ATSA Section 11, eval = FALSE, echo = FALSE}
```

### Indigenous Statistics book notes continued

### Applied Time Series Analysis Winter 2021 Class Notes

-   Don't include co-linear covariates for MARSS models (multivariate Autoregressiv State Space - same as Vector Autoregressive)
-   

## VAST project model runs, notes & plans

#### 5/18/2021

-   Set up code to create input objects (with POP only for now):

```{r VAST set up}
## Load packages and set working directory
library(TMB)               
library(VAST)
library(here)
library(dplyr)

# current VAST version this code was written for: 3.5.2
# current TMB version this was written for: 1.7.18

## Set species 
Species <- c("Sebastes alutus", "Sebastes polyspinis", "Pleurogrammus monopterygius")[1] 

## Set up folder to store results
results_folder <- paste0(here("results"), "/", Species[1])
dir.create(results_folder)

## Load raw survey data
raw_haul <- read.csv(here("data", "haul.csv"))

## Define stock names & NOAA ID numbers
stock_names <- c("Pacific Ocean Perch" = "Sebastes alutus", "Northern Rockfish" = "Sebastes polyspinis", "Atka Mackerel" = "Pleurogrammus monopterygius")
my_stock_ids <- c("30060", "30420", "21921")
names(my_stock_ids) <- stock_names

# Pre-processed data provided by Cecilia \
CPUE_data <- read.csv(here("data/CPUE.csv"))
BM_Stratum_data <- read.csv(here("data/BIOMASS_STRATUM.csv"))

# Separating out one species, POP is the first stock
CPUE_data <- CPUE_data[CPUE_data$SPECIES_CODE == my_stock_ids[1],]

# for POP species, start at 1990, for northern rockfish start at 1984:
CPUE_data <- CPUE_data[CPUE_data$YEAR >= 1990, ] # for POP
# CPUE_data <- CPUE_data[CPUE_data$YEAR >= 1984, ] # for northern rockfish

# Adding in start longitude and latitude from the raw_haul data frame into the CPUE_data data frame
CPUE_data <- merge(CPUE_data, raw_haul[, c("HAULJOIN", "START_LATITUDE", "START_LONGITUDE")], by = "HAULJOIN")

## Separating out the data for VAST input object, which is named Data_Geostat, and has to have these columns
Data_Geostat <- CPUE_data[, c("WEIGHT", "YEAR", "EFFORT", "START_LATITUDE", "START_LONGITUDE")]

# Renaming column names to the ones required by VAST
colnames(Data_Geostat) <- c("Catch_KG", "Year", "AreaSwept_km2", "Lat", "Lon")
```

-   Two versions of the beta term (temporal coefficient) in RhoConfig setting

    -   beta_1 and beta_2 = 0, indicating that the model should treat time as a fixed intercept

    -   beta_1 and beta_2 = 4, indicating that the model should treat the temporal coefficient as an autoregressive 1 process (1 time step)

-   Two distributions for the error distribution that need to be tried out and tested to see which fits the data better, the gamma distribution and the Tweedie distribution; this is controlled by an argument in the make_settings function

    -   ObsModel = c(2, 1) is the gamma distribution for positive catch, delta model for encounter probability

    -   ObsModel = c(10, 2) is tweedie for positive catch and tweedie link for encounter

-   When I run the model, some of the outputs go into the folder that I name, but all of the plots go into the folder that has the script in it. Look deeper in the code to figure out why this is happening and see if I can do a work around and/or reach out to Cecilia and Jim to let them know that this is happening so they can fix it in the next update. Maybe create a github issue about it?

-   Update VAST to most recent edition before I do anything else. My current code (with betas = 4, and set to gamma dist) is:

```{r VAST code 5.18.21}
# Defining the 3 sub-regions
strata.limits <- data.frame(STRATA = as.factor(c("Western", "Central", "Eastern")),
                            west_border = c(-Inf, -159, -147),
                            east_border = c(-159, -147, Inf))

## Spatial & spatiotemp random effects settings - ***I think these stay the same…
FieldConfig <- matrix( c("IID","IID","IID",
                         "IID","IID","IID"), #"IID" = independent 
                       ncol=2, nrow=3, 
                       dimnames=list(c("Omega","Epsilon","Beta"), 
                                     c("Component_1","Component_2")) ) 

#omega = spatial factor for encounter[1] & positive catch[2]
#epsilon = spatiotemp factor for encounter[1] & positive catch[2]

#beta = number of factors for intercepts (relevant for multivariate models - factor analysis for years)

## Temporal settings  
RhoConfig_betas <- c(4, 4) #run two versions: 0,0 for intercept and 4,4 for AR1 process

RhoConfig  <- c("Beta1" = RhoConfig_betas[1], "Beta2" = RhoConfig_betas[2], "Epsilon1" = 0, "Epsilon2" = 0)

## beta = autocorr for ecounter [1] & positive catch [2], zero is fixed effect intercept
## epsilon = spatiotemp variation for encounter[1] & positive catch[2]

settings = make_settings( Version = "VAST_v8_2_0", #.cpp version, not software #e.g., "VAST_v12_0_0"
                          n_x = 500, #knots aka spatial resolution of our estimates
                          Region = "User", 
                          purpose = "index2", #changes default settings
                          ObsModel= c(2,1), #c(1,1) #c(10,2); (2,1) is gamma for positive catch, delta model for encounter, and (10, 2) is tweedie for positive catch and tweedie link for encounter
                          ## everything after this is default if you use purpose = "index2"##
                          FieldConfig = FieldConfig, #spatial & spatiotemp random effects 
                          RhoConfig = RhoConfig, #temporal settings; default is all 0s, but if I specify this it will be changed here
                          strata.limits = strata.limits, #define area that you're producing index for
                          "knot_method" = "grid", #knots in proportion to spatial domain #other option is knot_method="samples"
                          fine_scale = TRUE, #changes the type of interpolation in your extrapolation area
                          bias.correct = TRUE, #corrects the index for nonlinear transformation; I want this for the final version, but I can turn it off while I'm messing with the model so it will run faster
                          use_anisotropy = TRUE) ##correlations decline depend on direction if this argument is TRUE

## Import extrapolation grid, these are available on Jason's Google drive: VASTGAP\Extrapolation Grids
GOAgrid <- read.csv(file= paste0(here("/data"),"/Extrapolation_Grids/GOAThorsonGrid_Less700m.csv"))

input_grid <- cbind(Lat = GOAgrid$Lat,
                    Lon = GOAgrid$Lon,
                    Area_km2 = GOAgrid$Shape_Area/1000000)  # Extrapolation grid area is in 
                                                            # m^2 & is converted to km^2

gc() #garbage collector

## Run model

fit <- fit_model( "settings"= settings, #all of the settings we set up above
                 "Lat_i"= Data_Geostat[,'Lat'], #latitude of observation
                 "Lon_i"= Data_Geostat[,'Lon'],  #longitude of observation
                 "t_i"= Data_Geostat[,'Year'], #time for each observation
                 "c_i"= rep(0,nrow(Data_Geostat)), #categories for multivariate analyses; don't actually use this, could comment it out if it doesn't have a fit
                 "b_i"= Data_Geostat[,'Catch_KG'], #in kg, raw catch or in CPUE per tow
                 "a_i"= Data_Geostat[,'AreaSwept_km2'], #sampled area for eahc observation
#                 "v_i"= Data_Geostat[,'Vessel'], #ok to leave in because it's all "missing" in data, so no vessel effects
                 "input_grid"= input_grid, #only needed if you have a user input extrapolation grid
                 "optimize_args" =list("lower"=-Inf,"upper"=Inf), #TMB argument (?fit_tmb)
                 "working_dir" = results_folder)

## Plot results
plot( fit )

## ## 
## save the VAST model
saveRDS(fit,file =  paste0(results_folder, "/", Species,"VASTfit.RDS"))
```

#### 5/20/2021

I'm updating VAST to the most recent version, which is: 3.7.1

-   Tried to run the gamma version (which worked with the 3.5.2 version) and got this error message: Error in (function (b_i, a_i, t_i, c_iz = rep(0, length(b_i)), e_i = c_iz[, : `X1_formula` and `X2_formula`; to use these please use `Version='VAST_v12_0_0'` or higher

    -   Figured out that this was referring to an argument in the make_settings() function, so I made it Version = "VAST_v12_0\_0" (it was previously Version = "VAST_v8_2\_0", in the run above)

-   The second attempt to run the gamma version has been going quite a bit longer than when I previously ran it (about 30 - 40 minutes) - I think I'll wait another hour (its been about an hour now) before stopping it. There haven't been any error messages, although this warning did appear a few lines ago: Note that \`getReportCovariance=FALSE\` causes an error in \`TMB::sdreport\` when no ADREPORTed variables are present

    -   The final line that is currently showing is: Bias correcting 90 derived quantities

    -   It finished, and produced the appropriate objects and plots, so I think it worked fine?

-   First attempt to run with the Tweedie distribution for errors (with ObsModel= c(10,2) in the make_settings function); seems to be going well so far.

    -   As the coefficients are scrolling by (presumably working on convergence?), and occasional message that says NA/NaN function evaluation appears, which didn't happen during any of the gamma runs. Check to see if this is anything to worry about with Cecilia.

    -   It is still running and seems okay, but it is definitely taking longer than the gamma versions - I've added code to record the run time for this script in the future, so I can actually know how long each script takes rather than having just a vague idea.

    -   This message also appears periodically, although it seems to be a general warning rather than a reaction to something in my code: Note that \`getReportCovariance=FALSE\` causes an error in \`TMB::sdreport\` when no ADREPORTed variables are present

    -   The model finished running, and produced all of the result objects and plots that I would expect, although they saved in several seemingly random locations, and some of the plots were named with "Plots" and then the plot title, not sure where that is coming from.

#### 5/21/2021

-   Tried running the model with tweedie distribution and betas = 4 (ObsModel = c(10,2) in make_settings function and "Beta1" = 4 and "Beta2" = 4 in RhoConfig object); it threw this error after starting to run the model and stopped:

```{r 5.21.21 VAST run message}
### Making TMB object
Note: Using Makevars in /Users/kellymistry/.R/Makevars 
make: Nothing to be done for `all'.
Constructing atomic tweedie_logW
Constructing atomic tweedie_logW
List of estimated fixed and random effects:

### Testing model at initial values
Error in if (any(Gradient0 == 0)) { : 
  missing value where TRUE/FALSE needed
```

-   Ask Cecilia what this means, and how to fix it.

    -   Cecilia's response: So it's not a huge issue, but that error implies that the gradient at one of the starting values of that model is 0, so the easiest fix is trying out a new starting value for whatever the issue parameter is, and this can involve some debugging to try and figure out which parameter it is. given that it looks like all that changed between the ones that run and the one that doesnt is the use of the tweedie distribution (I think?) I'd guess it's one of the tweedie model parameters. I'll try and come up with something to show you how to change the starting parameter values, but if you have time to look around it's usually in the fit_model() function.... you have to use some TMB functionality to get at the starting values.

    -   Basically what you need to do to alter parameter starting values is to run 

        > fit \<- fit_model(... , "run_model" = FALSE, ... )

        and then run as an example:

        > fit\$tmb_list\$Obj\$par['lambda1_k']  \<- 0.5

        where 'lambda1_k' will be whatever the parameter is that's causing issues. if you just run  fit\$tmb_list\$Obj\$par you can see all the parameters names and their starting values.

        then you just rerun the fit_model() argument like you normally would (so run_model = TRUE now, the default)

-   I'm going to run the gamma version again, with betas = 0, both to get the run time and to see if I was able to fix the weird file naming things that were happening in the previous tweedie/betas=0 run yesterday

    -   Results from the timer: user 4612.775 system 326.134 elapsed 4965.650. So, 1 hour and 20 minutes, approximately

    -   The weird file name issue is fixed now

-   I also moved the pieces that I keep changing to the top code chunk, so the pieces that might need to be changed are all in one place.

#### 5/26/2021

-   Troubleshooting which parameter is failing to converge in the tweedie/beta_4s version of the model:

    ```{r troubleshooting_5_26_21}
    # After running the fit function with "run_model" = FALSE so it doesn't actually run yet, these are the parameter starting values:
    fit$tmb_list$Obj$par 
      ln_H_input   ln_H_input   L_omega1_z L_epsilon1_z    L_beta1_z    logkappa1 Beta_mean1_c  Beta_rho1_f 
       0.0000000    0.0000000    1.0000000    1.0000000    1.0000000   -0.1053605    0.0000000    0.0100000 
      L_omega2_z L_epsilon2_z    L_beta2_z    logkappa2 Beta_mean2_c  Beta_rho2_f    logSigmaM 
       1.0000000    1.0000000    1.0000000   -0.1053605    0.0000000    0.0100000    1.6094379 
    ```

-   Tried each of these by themselves:

    -   fit\$tmb_list\$Obj\$par[' ln_H\_input'] \<- 0.5; same error

    -   fit\$tmb_list\$Obj\$par['Beta_mean1_c'] \<- 0.5; same error

    -   fit\$tmb_list\$Obj\$par['Beta_mean2_c'] \<- 0.5; same error

    -   fit\$tmb_list\$Obj\$par['Beta_rho1_f'] \<- 0.5; same error

    -   fit\$tmb_list\$Obj\$par['Beta_rho2_f'] \<- 0.5; same error

    -   fit\$tmb_list\$Obj\$par['Beta_rho1_f'] \<- 0.001; same error

    -   fit\$tmb_list\$Obj\$par['L_omega1_z'] \<- 0.5; same error

    -   fit\$tmb_list\$Obj\$par['L_epsilon1_z'] \<- 0.5; same error

    -   fit\$tmb_list\$Obj\$par['Beta_rho1_f'] \<- 1; same error

    -   

#### 6/2/2021

-   Still haven't gotten the tweedie/beta 4 model to run for POP, so I'm switching to running the models with Northern Rockfish

    -   There were 3 rows with NA values for AreaSwept_km2 - removed these and set it up to automatically check for NAs and delete the rows if there are less than 5 (more than 5 will stop the script and give a warning)

    -   Run time for gamma/beta 0s:

        -   user 4923.311 system 472.596 elapsed 5458.770; 1 hour and 30 minutes total

    -   Run time for gamma/beta 4s:

        -   user 4410.585 system 391.235 elapsed 60403.556; 16 hours and 46 minutes

            -   This one ran overnight, and I'm not sure why it was so much longer than for POP...

    #### 6/3/2021

    -   Tried to run tweedie/beta 0s, and got the same error as for tweedie/beta 4s with POP:

        -   \#\#\# Testing model at initial values Error in if (any(Gradient0 == 0)) { : missing value where TRUE/FALSE needed

        -   Starting parameters are:

            ```{r troubleshooting_NR_Tweedie_beta-0s}
            # parameter starting values:
            ln_H_input   ln_H_input     beta1_ft     beta1_ft     beta1_ft     beta1_ft 
               0.0000000    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
                beta1_ft     beta1_ft     beta1_ft     beta1_ft     beta1_ft     beta1_ft 
               0.0000000    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
                beta1_ft     beta1_ft     beta1_ft     beta1_ft     beta1_ft     beta1_ft 
               0.0000000    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
              L_omega1_z L_epsilon1_z    logkappa1     beta2_ft     beta2_ft     beta2_ft 
               1.0000000    1.0000000   -0.1053605    0.0000000    0.0000000    0.0000000 
                beta2_ft     beta2_ft     beta2_ft     beta2_ft     beta2_ft     beta2_ft 
               0.0000000    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
                beta2_ft     beta2_ft     beta2_ft     beta2_ft     beta2_ft     beta2_ft 
               0.0000000    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
                beta2_ft   L_omega2_z L_epsilon2_z    logkappa2    logSigmaM 
               0.0000000    1.0000000    1.0000000   -0.1053605    1.6094379 

            ```

        -   There seem to be quite a lot of parameters, and especially a lot of duplicate parameters, beta1_ft and beta2_ft - is that right?

            -   Running tweedie/beta 0s for POP again, to see how many parameters are in that one and if it matches the above (also to get run time, since I didn't do that the first time)

                -   Run time: user 8559.819 system 430.029 elapsed 9049.576; 2 hours and 30 minutes

                -   Parameters do look similar, with multiple beta1_ft and beta2_ft parameters

    ```{r POP_tweedie_beta-4s}
    fit$tmb_list$Obj$par
      ln_H_input   ln_H_input     beta1_ft     beta1_ft     beta1_ft     beta1_ft 
       0.0000000    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
        beta1_ft     beta1_ft     beta1_ft     beta1_ft     beta1_ft     beta1_ft 
       0.0000000    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
        beta1_ft     beta1_ft     beta1_ft     beta1_ft   L_omega1_z L_epsilon1_z 
       0.0000000    0.0000000    0.0000000    0.0000000    1.0000000    1.0000000 
       logkappa1     beta2_ft     beta2_ft     beta2_ft     beta2_ft     beta2_ft 
      -0.1053605    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
        beta2_ft     beta2_ft     beta2_ft     beta2_ft     beta2_ft     beta2_ft 
       0.0000000    0.0000000    0.0000000    0.0000000    0.0000000    0.0000000 
        beta2_ft     beta2_ft     beta2_ft   L_omega2_z L_epsilon2_z    logkappa2 
       0.0000000    0.0000000    0.0000000    1.0000000    1.0000000   -0.1053605 
       logSigmaM 
       1.6094379 
    ```

    -   Tried to run tweedie/beta 4s, and got the same error:

        -   \#\#\# Testing model at initial values Error in if (any(Gradient0 == 0)) { : missing value where TRUE/FALSE needed

        -   Trying different starting parameters:

            -   fit\$tmb_list\$Obj\$par['Beta_rho1_f'] \<- 0.5; same error

            -   fit\$tmb_list\$Obj\$par['Beta_rho2_f'] \<- 0.5; same error\

## Random walk model runs, notes & troubleshooting

#### 6/7/21

-   Working on creating a random walk model script based on the code I got from Kristin, which uses ADMB. I need to check with Cecilia whether it makes a difference if I use ADMB or TMB (I think not significantly, maybe TMB is faster though?), but I want to see if I can get some kind of result from this code today before contacting her

-   I reached out to Kristen about what columns her input data file had ("dat" argument in her function), and she sent me one of her files. The columns are: survey, year, regulatory area, biomass, variance, SE, CV, haul count, catch count and group (because she's working with species that are grouped together for management purposes, so I would have species instead of group). In her email, she said: "The data files are going to be specific to your species. Mine are calculated simply (see attached file), but yours will mostly likely be calculated based on a different model (probably ask the lead assessment author how they calculate those values for your species)."

    -   So I guess I need to ask Cecilia and/or look at the stock assessments to figure this out. Each GOA region has 1 entry per year, so these are clearly calculated values from the raw survey data and I need to know what the model is that was used to calculate them.

    -   I'm checking the SAWG paper to see if the model equations are spelled out here - no such luck

#### 6/11/21

-   Reformatted VAST code in order to run it with less effort while troubleshooting

    -   This meant splitting some things into their own scripts (functions, and initial data formatting)

-   Created a function to systematically test new initial parameter values

    -   First used it for northern rockfish tweedie/betas 4, with 0.5 as the new initial value for each parameter

        -   All of the parameters with value of 0.5 fail to converge with the same error message

    -   Now trying with each parameter = -0.5

        -   All failed to converge

    -   Now trying with each parameter = 10 (just for the hell of it)

        -   All failed to converge

#### 6/15/21

-   After discussing the various roadblocks to both the random walk and VAST model runs with Cecilia via email yesterday, I am prioritizing the gamma version of the model and will set aside the tweedie ones for now. The gamma/beta 0s version worked for the northern rockfish, but not for the gamma/beta 4s version, so I'm trying that one again now.

    -   It seems to be working. Not sure why it didn't converge last time, since nothing about the starting values has changed - not going to argue with success though.

    -   Run time for gamma/beta 4s for northern rockfish is:

        -   user 4645.575 system 435.154 elapsed 6169.533; 1 hour and 45 minutes, total

-   For the random walk roadblock (missing an intermediate calculated step for summing in each year), Cecilia was able to send me this because her team is the one who does this calculation for the stock assessment modelers. Basically, its a sum of the survey catch divided by the area surveyed, but she just sent me the calculated values so I don't have to do it myself. This is a step I want to be sure is clear in my methods description though, as it would be required to recreate the random walk method if anyone wanted to do it. If the above VAST model finishes running relatively soon today, I may have time to see if I can try running this data through the random walk code. Otherwise I'll do it tomorrow.

#### 6/16/21

-   Working on getting the random walk model to run with the data file that Cecilia provided, using Curry's run_re_model function.

    -   No luck - can't figure out the "DateFile" input the function requires for in Curry's code

#### 6/21/21

-   Emailed Kristen with questions that will hopefully let me run her code - she is in crunch time trying to finish her thesis though, so she may not have time to answer before Wednesday. I should also email Curry, although maybe I'll email Cecilia first.

    -   After 2 hours of working with Mark, I got ADMB installed properly (I think), but am still having issues running the model. It compiles successfully, but I get this error when I run run_admb(): "Error: Invalid index 1 used for array range [1984, 2019] in "df1b2variable& df1b2vector::operator () (int i) const".

        Index too low"

        -   This looks like a C++ error message, but I'm not sure what's causing it - I *think* it's the num_idx variable, which Curry says is the "number of survey indices", but it gives the same error message both with 1 (the number of species) or 16 (the number of years)

#### 6/24/21

-   Yesterday I reinstalled ADMB yet again (again after trying several different ways - the one that eventually worked was to [use the source zip](http://www.admb-project.org/downloads/admb-12.2/BuildingSourceUnix.html) and do all of the commands in the terminal), and finally got it to the point that I was able to successfully run the simple example

-   Today, I tried running the RE model with Curry's re.dat file from his github, and the good news is that ADMB appears to have done what it was supposed to do, by successfully compiling the re.tpl file and attempting to run the model.

    -   To run ADMB from the terminal, all I seem to have to do is navigate to the admb-12.2 folder:

        -   **cd admb-12.2**

-   The bad news is that it was unsuccessful, and came up with the following error, which is similar to the ones I was getting with my re.dat file: "Error: Invalid index 1987 used for array range [1, 1984] in "df1b2variable& df1b2vector::operator () (int i) const". Index too high" - the errors I was getting said "Index too low", but otherwise its pretty much the same. So I may have to contact Curry after all. Alternatively, I can try what Cecilia suggested, which is putting print code after each step in the .tpl file, so I can figure out where the problem is happening.

    -   I tried running Curry's files through terminal (his [re.dat](https://github.com/curryc2/AFSC_VAST_Evaluation/blob/develop/admb/re.dat) and [re.tpl](https://github.com/curryc2/AFSC_VAST_Evaluation/blob/develop/admb/re.tpl)), not R. Here is the code to do that, navigating from kellymistry/admb-12.2:

        -   changing directories to get to the one that has the re.dat and re.tpl files

            -   **cd examples/admb/re**

        -   compile the re.tpl file to build an executable

            -   **admb re.tpl**

        -   run the model

            -   **./re**

-   Just for fun, I tried running my re.dat file in the terminal, and got basically the same error message as above, although it is slightly different from the error messages I was getting before: "Error: Invalid index 16 used for array range [1, 1] in "df1b2variable& df1b2vector::operator () (int i) const". Index too high"

    -   The current version of my re.dat file looks like this:

        \# "/Users/kellymistry/Desktop/Raw Data/NOAA/groundfish_RE_model//re.dat" produced by dat_write() from R2admb Tue Jun 22 13:06:33 2021

        \#

        1984

        \#

        2019

        \#

        1

        \#

        1

        \#

        16

        \#

        1993 1996 1999 2001 1990 2015 2017 1987 2011 1984 2013 2009 2019 2005 2007 2003

        \#

        483622.5 771412.6 727063.5 673155 157295.2 1140407 1570359 241205.8 778669.7 220672.3 1298443 649448.8 1212145 764900.8 688179.8 457421.5

        \#

        0.0003102719 0.0005813243 0.001109696 0.0006340129 0.0001345956 0.0005049531 0.001000155 0.0001566268 0.0003869881 0.0001601923 0.0006097751 0.0003363097 0.0004685531 0.0004187516 0.0003260615 0.0002056652

#### 6/29/21

-   Emailed Kristen on 6/28 to see when she might be available to go over her code, haven't heard back yet. Got GOA indices data separated by subregion from Cecilia (since it originally was for the whole region, and I needed it by subregion).

-   Started working on figuring out how Curry's re.tpl C++ code works, using his re.dat file and putting print statements into the re.tpl code to see how the data is being read in, and how the objects used in the model are created from that data. The error message is: *Error: Invalid index 1987 used for array range [1, 1984] in "df1b2variable& df1b2vector::operator () (int i) const". Index too high"* The print statement I used is std::cerr, with this format with the object styr as an example:

    -   std::cerr \<\< "start year is" \<\<styr;

    -   Through trial and error, I found that this line needs to go in the Procedure Section, under the first line (and it needs to be indented once, so in line with the rest of the code there.

    -   These are the values for the objects created from the data:

        -   styr = 1984 - makes sense

        -   endyr = 2015 - makes sense

        -   yrs = c(1984:2015) - not totally sure how this object is used in the code, but there aren't that many values for biomass because surveys didn't happen every year, so maybe makes sense, maybe not?

        -   num_indx = 13 - this is the number of years - I think this is right, but not 100% sure

        -   n_PE = 1984 - this is definitely wrong, because this is the number of process error parameters, and the default is 1 in Curry's code

        -   PE_vec = 1987 1990 1993 1996 1999 2003 2005 2007 2009 2011 2013 2015 2762 - this is also definitely wrong, for one thing because its the rest of the year string and the first one of the next string, which I'm almost completely certain is the biomass numbers

        -   nobs = 4507 - also definitely wrong, as this is the number of surveys, which I'm pretty sure should be 13 (the number of years), based on the comments in Curry's function, and also this is the second value in the biomass string

        -   yrs_surv = 4034 8447 4885 1873 6251 4655 11681 8415 4040 9675 13693 and 0s for 4507 elements (because nobs is wrong) -

        -   srv_est = 0 x 4507, again because nobs is wrong

        -   srv_cv = 0 x 4507, again because nobs is wrong

    -   n_PE seems to be where it goes wrong - I'm going to try messing with the re.dat file to see if I can get this to be 1, which is the default so hopefully that would produce something

    -   I'm not sure what exactly the PE_vec object is - the name makes me think that its a vector of process error parameters, but the defaults in Curry's code for number of process error parameters is 1, while PE_Vec is c(1,1,1), so...not sure what that means. And in re.tpl, the PE_vect object is initialized as a vector of length num_idx, so that means 13...which I guess makes sense because there would be a process error for each year. But then why is the default n_PE = 1? Maybe need to talk this through with Cecilia.

-   This is the re.dat content that I tried running, taking the above into account:

1984

2015

13

1

1 1 1 1 1 1 1 1 1 1 1 1 1

13

1984 1987 1990 1993 1996 1999 2003 2005 2007 2009 2011 2013 2015

2762.7 4507 4034 8447.6 4885.2 1873.8 6251.8 4655.2 11681.6 8415.7 4040.5 9675.6 13693.4

0.153405 0.3375416 0.1731722 0.1259427 0.1394823 0.1283734 0.2957936 0.1826974 0.2044163 0.1581157 0.1341935 0.1573927 0.1232839

-   The initial index error didn't happen, which is progress - there were several loops that occurred before this error was thrown: Error in matrix inverse -- matrix singular in inv(dmatrix)

    -   This is the parameters that were printed in those loops:

        -   start year is 1984

            end year is 2015

            year vector is  1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015

            number of indices is 13

            Number of process error parameters is 1

            PE_vec is  1 1 1 1 1 1 1 1 1 1 1 1 1

            Numbers of surveys is 13

    -   I adjusted the print statements to print survey years, biomass estimates, and biomass CV objects:

        -   Survey years is  1984 1987 1990 1993 1996 1999 2003 2005 2007 2009 2011 2013 2015

        <!-- -->

            Biomass estimate is  2762.7 4507 4034 8447.6 4885.2 1873.8 6251.8 4655.2 11681.6 8415.7 4040.5 9675.6 13693.4

             0.153405 0.337542 0.173172 0.125943 0.139482 0.128373 0.295794 0.182697 0.204416 0.158116 0.134193 0.157393 0.123284

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

            Biomass CV is  0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

             0 0 0 0 0 0 0 0 0 0 0 0 0

    -   So there's clearly an issue the biomass estimate and biomass CV having too many elements. In the re.tpl, this is how they are defined:

        -   srv_est(1,nobs,1,num_indx)

        -   rv_cv(1,nobs,1,num_indx)

        -   So these are 13 x 13 matrices, and they should be 1 x 13. I'm guess that the num_idx is the one that should be 1, since nobs is by definition the number of survey years.

        -   I also really think that the number of process error parameters should be 13, so I changed that too.

    -   I made num_idx = 1, n_PE = 1, and PE_vec = 1 (even if that might be wrong), so the re.dat looks like this:

        -   1984

            2015

            1

            1

            1

            13

            1984 1987 1990 1993 1996 1999 2003 2005 2007 2009 2011 2013 2015

            2762.7 4507 4034 8447.6 4885.2 1873.8 6251.8 4655.2 11681.6 8415.7 4040.5 9675.6 13693.4

            0.153405 0.3375416 0.1731722 0.1259427 0.1394823 0.1283734 0.2957936 0.1826974 0.2044163 0.1581157 0.1341935 0.1573927 0.1232839

        -   And the model seems to have run and produced something. This is what the current printing statements returned (and I think all of this is what they should be):

            -   number of indices is 1

                Number of process error parameters is 1

                PE_vec is  1

                Numbers of surveys is 13

                Survey years is  1984 1987 1990 1993 1996 1999 2003 2005 2007 2009 2011 2013 2015

                Biomass estimate is  2762.7

                 4507

                 4034

                 8447.6

                 4885.2

                 1873.8

                 6251.8

                 4655.2

                 11681.6

                 8415.7

                 4040.5

                 9675.6

                 13693.4

                Biomass CV is  0.153405

                 0.337542

                 0.173172

                 0.125943

                 0.139482

                 0.128373

                 0.295794

                 0.182697

                 0.204416

                 0.158116

                 0.134193

                 0.157393

                 0.123284

        -   And the biomA.out output seems to have biomass estimates for all of the years, plus one more years

```{r Curry results}
biomA_Curry_results_6.29.21 <- c(2815.47, 3201.96, 3641.51, 4141.39, 4160.69, 4180.07, 4199.55, 5232.38, 6519.21, 8122.53, 6822.58, 5730.68, 4813.53, 3581.34, 2664.57, 1982.48, 2544.91, 3266.90, 4193.71, 5383.46, 5213.03, 5047.99, 7220.64, 10328.40, 9182.36, 8163.48, 5972.73, 4369.89, 6401.93, 9378.90, 11235.40, 13459.50)

survey_years <- c(1984:2015)

plot(survey_years+1, biomA_Curry_results_6.28.21, xlab = "", ylab = "")
```

#### 6/30/21

-   Cecilia sent the version of the calculated data separated out by subregion, and I was able to successfully run it through Curry's code for the western subregion, and got results.

```{r western_results}
biomA_western_results_6.30.21 <- c(60666.0, 61886.8, 63132.2, 64402.6, 46692.0, 33851.8, 24542.6, 35702.9, 51938.1, 75556.0, 80862.0, 86540.5, 92617.9, 68809.6, 51121.4, 37980.2, 102237.0, 275209.0, 141595.0, 72850.6, 135201.0, 250915.0, 199172.0, 158100.0, 70837.5, 31739.1, 56169.8, 99405.8, 125109.0, 157457.0, 143272.0, 130364.0, 159287.0, 194627.0, 91542.1, 43056.5)
plot(1984:2019+1, biomA_western_results_6.30.21, xlab = "", ylab = "")

```

-   It's great that this code works, but it's not actually the admb code that is used anymore, there have been some changes. So now I need to figure out the differences between Curry's re.tpl script and the one I got from the official git repository, and I can figure out what I need to adjust in my data file to get that one to run.

    -   It looks like, as far the input data, there are fewer variables, just start year, end year, nobs (number of survey years), survey years, biomass estimates, and biomass CVs (I think)

    -   It seems to have worked! Now I have to figure out how to use the outputs. I'll go back to Kristen's code to see what she does.

#### 7/6/21

-   Now that the RE model is working, my next steps are:

    -   Clean up RE code, put it in in its own script and push to github (this week) - completed on 7/14

    -   Do RE model runs for the rest of the subregions for POP and all subregions for NR (this week) - completed on 7/12

    -   Update Cecilia about progress and planned next steps (this week) - completed on 7/7

    -   Write equation for relative error metric, and run that by Mark and Cecilia to check it (this week or next week) - created 2 versions, emailed Cecilia for opinion on 7/20

    -   Write up methods up to this point - be very detailed (starting next week)

    -   Implement both relative error and MASE metrics with both RE and VAST results (start next week, alternating between this and writing methods)

    -   Get simulation code from Cecilia and start working on adapting that for my data (start after done with implementing metrics - might coincide with still writing methods; ideally starting in no less than 3 weeks from now)

#### 7/12/21

-   Still working on cleaning up the RE model code.

-   Ran the rest of the subregions for both species, and the eastern/sebastes_polyspinis has 0 values for catch, which makes the CV an NA, and therefore the RE model can't be calculated - check with Mark or Cecilia about how to deal with this. - emailed Cecilia on 7/20

#### 7/14/21

-   Created a git repository for RE model and uploaded all of the relevant script, data and results files

-   Description from 2013 unpublished paper, that I need to replicate: "Statistics selected to evaluate the performance of the various methods include the mean relative error (MRE) of biomass (relative error is defined here as estimate/true-1) and variability in these relative errors."

    -   Using the same notation as my equation of mean absolute scaled error (MASE) in my proposal (with $P_n$ = predicted value for observation *n* and $O_n$ = observed value for observation *n*):

$$
\text{MRE} = \frac{1}{N}\sum_{n = 1}^N \frac{P_n}{O_n - 1}
$$

-   Alternatively, based on looking up what relative error equations usually look like, would it be this?

    $$
    \text{MRE} = \frac{1}{N}\sum_{n = 1}^N \frac{\left| P_n - O_n \right|}{O_n - 1}
    $$

-   Not sure what "variability of these relative errors" means exactly - I assume the distribution of this $N$ length vector of RE values? Variance, etc.

$$
\text{RE} = \frac{P_n}{O_n - 1}
$$

OR

$$
\text{RE} = \frac{\left| P_n - O_n \right|}{O_n - 1}
$$

#### 7/15/21

-   Still need to check with Mark and/or Cecilia about which of the above equations is better. In the meantime, I'll work on coding both versions. - coding both versions, and MASE, completed on 7/20, and I sent Cecilia the above questions on 7/20, will ask Mark in our meeting on 7/21

#### 7/21/21

-   Topics for meeting tomorrow

    -   Update on progress & questions

        -   RE model run successfully, have results for all but NR Eastern

            -   Include some visuals - maybe with both RE & VAST prediction results plotted on the same graph for visual comparison?

        -   Evaluation metrics coded, run with POP Western so far

            -   MASE and MRE

                -   Ask if Paul or Jim knows if either version I have is what was used in the 2013 paper; also, if they don't know, if there's much of a difference between them, and if it matters which I use

                -   Ask if I'm interpreting "variability of these relative errors" correctly

    -   Next steps

        -   Run evaluation metrics with the rest of the species/subregions

        -   Start working on the simulation code

        -   Work on writing up the methods
